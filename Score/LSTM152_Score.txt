Processed 1000 ...
Processed 2000 ...
Processed 3000 ...
Processed 4000 ...
Processed 5000 ...
Processed 6000 ...
Processed 7000 ...
Processed 8000 ...
Processed 9000 ...
Processed 10000 ...
Processed 11000 ...
Processed 12000 ...
Processed 13000 ...
Processed 14000 ...
Processed 15000 ...
Processed 16000 ...
Processed 17000 ...
Processed 18000 ...
Processed 19000 ...
Processed 20000 ...
Processed 21000 ...
Processed 22000 ...
Processed 23000 ...
Processed 24000 ...
Processed 25000 ...
Processed 26000 ...
Processed 27000 ...
Processed 28000 ...
Processed 29000 ...
Processed 30000 ...
Processed 31000 ...
Processed 32000 ...
Processed 33000 ...
Processed 34000 ...
Processed 35000 ...
Processed 36000 ...
Processed 37000 ...
Processed 38000 ...
Processed 39000 ...
Processed 40000 ...
loading annotations into memory...
0:00:01.640496
creating index...
index created!
Loading and preparing results...     
DONE (t=0.28s)
creating index...
index created!
tokenization...
PTBTokenizer tokenized 2492288 tokens at 2267758.24 tokens per second.
PTBTokenizer tokenized 444860 tokens at 1378123.90 tokens per second.
setting up scorers...
computing Bleu score...
{'reflen': 393947, 'guess': [399918, 359414, 318910, 278406], 'testlen': 399918, 'correct': [267218, 128718, 55288, 23839]}
ratio: 1.0151568612
Bleu_1: 0.668
Bleu_2: 0.489
Bleu_3: 0.346
Bleu_4: 0.244
